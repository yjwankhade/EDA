# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yNeHWnU4nIdzvKv-jljC_uRcEJ0CU-mR
"""

!pip install missingno --quiet
!pip install plotly

#from google.colab import files
#uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
import os
import sys
import re
import numpy as np               # Performing Mathametical opration
import pandas as pd              # Data processing and dataframe
import matplotlib.pyplot as plt  #Visualisation library
import seaborn as sns            # Visualisation library
import itertools                 # Library to iterate over items.
import missingno as msno         # Specialized Library for Missing Values
from scipy import stats          # Stats library
import plotly.express as px
import warnings as wrn
import plotly.graph_objects as go

wrn.filterwarnings('ignore', category = DeprecationWarning)
wrn.filterwarnings('ignore', category = FutureWarning)
wrn.filterwarnings('ignore', category = UserWarning)

pd.set_option('display.max_columns',None)  #Columns visibility setting
# pd.set_option("display.precision", 2)
pd.options.display.float_format = '{:.5f}'.format #5f diplay five decimal floating
# %config InlineBackend.figure_format = 'retina'  # Plotting pretty figures and avoid blurry images
# %matplotlib inline

df_mf = pd.read_csv("/content/Morningstar - European ETFs.csv",skipinitialspace = True)
df_etf = pd.read_csv("/content/Morningstar - European ETFs.csv", skipinitialspace = True)

df_mf_copy = df_mf.copy(deep = True)

"""jhjhh"""

# Understanding values of feature highlighting country exposure of fund
df_mf['country_exposure'].head(5)

# Observation
# Some funds have invested in multiple countries, so we need to split all countries from each row.

df_mf.head()

import plotly.graph_objects as go
mf_geo_copy = df_mf.copy()

# Extract country codes from each row in the 'country_exposure' column
mf_all_country_codes = mf_geo_copy['country_exposure'].str.findall(r'([A-Z]{3}):')

# Flatten the list of lists into a single list
mf_all_country_codes = [code for codes_list in mf_all_country_codes.dropna() for code in codes_list]

# Remove -99 from the list-Ignore
mf_all_country_codes = [code for code in mf_all_country_codes if code != '-99']

# Count the frequency of each country code
mf_country_code_counts = pd.Series(mf_all_country_codes).value_counts().reset_index().rename(columns={'index': 'Country Code', 0: 'Frequency'})

# Create a Geo Heatmap using Plotly
fig = go.Figure(go.Choropleth(
    locations=mf_country_code_counts['Country Code'],
    z=mf_country_code_counts['Frequency'],
    text=mf_country_code_counts['Country Code'],
    colorscale='Viridis',
    colorbar_title='Frequency of Country Codes',
    marker_line_color='white',  # Add white border lines for better visibility
    marker_line_width=0.5,
))

fig.update_layout(
    title_text='Geographic Investments in Fund Portfolios',
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection_type='natural earth',
        bgcolor='lightblue',  # Set background color
    ),
    margin=dict(l=0, r=0, b=0, t=40),  # Adjust margin for better layout
)

fig.show()

# To print all values of country and frequency associated with it.
# for index, row in mf_country_code_counts.iterrows():
# print(f"Country: {row['Country Code']}, Frequency: {row['Frequency']}")

etf_geo_copy = df_etf.copy()

# Extract country codes from each row in the 'country_exposure' column
etf_all_country_codes = etf_geo_copy['country_exposure'].str.findall(r'([A-Z]{3}):')

# Flatten the list of lists into a single list
etf_all_country_codes = [code for codes_list in etf_all_country_codes.dropna() for code in codes_list]

# Remove -99 from the list-Ignore
etf_all_country_codes = [code for code in etf_all_country_codes if code != '-99']

# Count the frequency of each country code
etf_country_code_counts = pd.Series(etf_all_country_codes).value_counts().reset_index().rename(columns={'index': 'Country Code', 0: 'Frequency'})

# Create a Geo Heatmap using Plotly
fig = go.Figure(go.Choropleth(
    locations=etf_country_code_counts['Country Code'],
    z=etf_country_code_counts['Frequency'],
    text=etf_country_code_counts['Country Code'],
    colorscale='Viridis',
    colorbar_title='Frequency of Country Codes',
    marker_line_color='white',  # Add white border lines for better visibility
    marker_line_width=0.5,
))

fig.update_layout(
    title_text='Geographic Investments in ETF Portfolios',
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection_type='natural earth',
        bgcolor='lightblue',  # Set background color
    ),
    margin=dict(l=0, r=0, b=0, t=40),  # Adjust margin for better layout
)

fig.show()

# To print all values of country and frequency associated with it.
# for index, row in etf_country_code_counts.iterrows():
#     print(f"Country: {row['Country Code']}, Frequency: {row['Frequency']}")



"""#Correlation between MF & ETF based on Data

"""

pd.concat([df_mf, df_etf], axis = 1, keys = ['df_mf', 'df_etf']).corr().loc['df_mf', 'df_etf'][0:10] #[Row][Columns]

"""Visual Representation of co-relation

why we establish co-relation?

The correlation coefficient is a statistical measure that quantifies the degree to which two variables move in relation to each other. It ranges from -1 to 1, with the following interpretations:

1: Perfect positive correlation. If one variable increases, the other variable also increases proportionally.

0: No correlation. There is no linear relationship between the two variables.

-1: Perfect negative correlation. If one variable increases, the other variable decreases proportionally.

"""

corr_matrix=pd.concat([df_mf, df_etf], axis = 1, keys = ['df_mf', 'df_etf']).corr().loc['df_mf', 'df_etf'].iloc[:10, :10] # Select necessary rows and column to visualize desired co-relation

plt.figure(figsize=(10,10))
sns.set(font_scale=0.8)
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap between mutual funds and ETF')
plt.show()

"""**Result**

1.   Initially, it seems like the features in both datasets share similar names, but there isn't a strong positive connection between them.

2.   This indicates that Mutual Funds (MF) and Exchange-Traded Funds (ETF) are distinct instruments with different characteristics.

3.  Key Differences:

    i.    Trading Approach: MFs trade at the closing net asset value, while ETFs trade throughout the day, causing value changes.

    ii. Tax Impact: Generally, MFs face higher taxes compared to ETFs, with some exceptions.

    iii. Liquidity: Mutual Funds are less liquid than ETFs.

    iv. Management Costs: MFs have higher expenses due to active management, while ETFs, tracking an index, require less hands-on management.

**Data Preprocessing and EDA**

DATA PREPROCESSING CHECKLIST

* **Segregating the target variable and segregating numerical and categorical variables**

* **Type Casting of Variables:**

  Ensure that variables have the correct data type. For example, converting numerical variables stored as strings to actual numeric types.


* **Handling Missing Values - Drop / Impute:**

  Dealing with missing data, either by removing rows or columns with missing values, or by filling in the missing values with imputation methods.


* **Handling Outliers - Drop / Capping, Flooring:**

  Addressing outliers in the data. This can involve removing extreme values, capping or flooring them (replacing extreme values with a predefined threshold).


* **Remove Duplicate Values:**

  Identifying and removing duplicate records in the dataset.


* **Feature Scaling of High-Range Variables:**

  Scaling numerical features, especially when they have different ranges, to ensure that they contribute equally to the analysis. Common scaling methods include normalization and standardization.


* **Categorical Encoding of Categorical Features:**

  Converting categorical variables into a numerical format that can be used by machine learning algorithms. Common techniques include one-hot encoding, label encoding, or binary encoding.


* **Feature Selection based on Multi-collinearity, Zero or Low Variance:**

  Selecting a subset of relevant features based on criteria such as multicollinearity (correlation between independent variables), zero or low variance (features with little variability).

**Segregating** Dataset


1.Segregating Target Variable from Predictors.

2.Segregating Numerical Variables from Categorical Variables.
"""

df_mf_num = df_mf.select_dtypes(include = 'number')
df_mf_cat = df_mf.select_dtypes(include = ['object','category'])

"""**What is Target Variable and Predictors:**

In a dataset, you typically have variables that you want to predict or explain. The variable you want to predict is called the **target variable**.

The other variables that you use to make predictions or estimates about the target variable are called **predictors or features**.

Nature of Target variable is Continous numbers.It is essentially, a Supervised Regression problem.


**For example**, if you're trying to predict the price of a house, the price would be the target variable, and predictors could include features like the number of bedrooms, square footage, location, etc.

---



---

**What is Numerical Variables and Categorical Variables.**

**Numerical Variables:**

Numerical variables represent measurable quantities and can take on numerical values, such as age, income, or temperature.
They can be either continuous (e.g., height) or discrete (e.g., number of siblings).

**Categorical Variables:**

Categorical variables represent categories or labels, like gender, color, or country.
They can be nominal (with no inherent order) or ordinal (with a meaningful order).

**Segregating Numerical Variables from Categorical Variables:**

Machine learning models often treat numerical and categorical variables differently, so it's crucial to segregate them during data preprocessing.
Numerical variables are usually used as-is, while categorical variables need to be converted into a numerical format through encoding methods like one-hot encoding or label encoding.
This segregation ensures that the model can appropriately handle and interpret both types of variables during training and testing.
"""

# Understanding MF dataframe -Help with initial understanding of Data
df_mf.sample(5)

# Initial Observation:
# 1.Many values of some features are missing in dataframe. i.e. Ratings.
# 2.Features like "ticker" and "isin" looks like identifier and Unique.-Will confirm later.
# 3."Investment strategy" has variable info and quite lenghty and complex-Will Check uniqueness or Drop. Can check investment styles are unique or not
# 30002 and numbers in exterme left are row nos.

display(df_mf.shape,
        df_mf_num.shape,
        df_mf_cat.shape)  # Helps in differentiating features as Numerical or Catergorical

# Observation
# In mutual fund data set-out of 132 features,110 are numerical and 22 are Categorial.
# Will Check which features are categorical and Numerical in Dataset.
# There might me instance of of "Numerical" feature being "Categorical"

# Summary of MF df  -Visual and detailed summary of features in categorical and Numerical data.
df_mf.info(verbose=False, memory_usage=False)
print("\n "+"Numerical variables")
display(df_mf.select_dtypes(include='number').describe())
print("\n"+"Categorial Variables")
display(df_mf.select_dtypes(include=['object', 'category']).describe())

# Observation

df_mf_num.nunique().sort_values(ascending = True)[0:15]
# Top 15 Features with unique values in ascending order

# Psudo Categorical Variables  - MF
# rating                                 5
# risk_rating                            5
# performance_rating                     5
# sustainability_rank                    5
# average_credit_quality                18
# quarters_down                         20
# quarters_up                           24
# sustainability_percentage_rank       100
# involvement_gmo                      162
# involvement_palm_oil                 197
# involvement_small_arms               386
# involvement_pesticides               386
# involvement_controversial_weapons    480
# involvement_gambling                 523

df_mf_cat.nunique().sort_values(ascending = True)[0:15]

print(df_mf_cat.columns.values)

# Observation:

# Categorical variables

#  ['ticker' 'isin' 'fund_name' 'inception_date' 'category' 'analyst_rating'
#  'investment_strategy' 'investment_managers' 'dividend_frequency'
#  'fund_benchmark' 'morningstar_benchmark' 'equity_style' 'equity_size'
#  'bond_interest_rate_sensitivity' 'bond_credit_quality' 'country_exposure'
#  'latest_nav_date' 'nav_per_share_currency' 'shareclass_size_currency'
#  'fund_size_currency' 'top5_regions' 'top5_holdings']

# Categorical variable with lesser class
# bond_credit_quality                  3
# bond_interest_rate_sensitivity       3
# equity_size                          3
# equity_style                         3
# dividend_frequency                   6
# analyst_rating                       6
# fund_size_currency                  14

# Pseudo Categorical Variables
# rating                                 5
# risk_rating                            5
# performance_rating                     5
# sustainability_rank                    5
# average_credit_quality                18
# quarters_down                         20
# quarters_up                           24
# sustainability_percentage_rank       100
# involvement_gmo                      162
# involvement_palm_oil                 197
# involvement_small_arms               386
# involvement_pesticides               386
# involvement_controversial_weapons    480
# involvement_gambling

"""Note:

1. Not all "Object" dtype variables are Categorical,Since no. of classes are way beyond to recognize as Categorical.

2. Conversly, not all "float","int" dtype varialbles are Numerical since it has no. classes to be qualify as Categorical variables.

3. To identify Categorical features with dtype "int","float" extract the no. of UNIQUE values from object,float and int variables.

4. Short-list the variables across all dtypes which has the no. of unique values qualified to be a Categorical variables.

5. Create a variable of such short-listed features for further EDA and pre - processing.
For convveinence purpose, I will create 4 (2 each for MF & ETF) new variables, 1 which have variables where unique values =< 20 and other where unique values > 20.

6. Variables Creating a new variable which contains features having no. of unique values < 20. - MF Creating a new variable which contains features having no. of unique values > 20. - MF

7. Creating a new variable which contains features having no. of unique values < 20. - ETF Creating a new variable which contains features having no. of unique values > 20. - ETF
"""

# Creating a new variable which contains features having no. of unique values < 20. - MF
mf_small_cat = df_mf[["morningstar_analyst_rating", "dividend_frequency", "equity_style", "equity_size" ,
                      "bond_interest_rate_sensitivity" , "bond_credit_quality", "fund_size_currency", "morningstar_rating",
                      "morningstar_risk_rating", "morningstar_performance_rating", "average_credit_quality", "sustainability_rank"]].copy()

display(mf_small_cat.shape,
mf_small_cat.size,
mf_small_cat.sample(5))

# Creating a new variable which contains features having no. of unique values > 20. - MF

mf_big_cat = df_mf[["nav_per_share_currency", "nav_per_share", "morningstar_benchmark", "involvement_palm_oil",
                    "involvement_gmo", "sustainability_percentage_rank"]].copy()#, '"quarters_up"'

mf_big_cat.shape
mf_big_cat.size
mf_big_cat.sample(5)

# Describing Categorical Variables  - MF
df_mf_cat.describe(include = "O").T

"""**Note into Categorical into -**

NOMINAL

ORDINAL

BINARY

1. There are No Binary features.
2. Following are Ordinal features

  i. bond_credit_quality

  ii. bond_interest_rate_sensitivity

  iii. equity_size

  iv. analyst_rating

  v. rating

  vi. risk_rating

  vii. performance_rating

  viii. sustainability_rank

  ix. average_credit_quality

  x. quarters_down

  xi. quarters_up

  xii. sustainability_percentage_rank

Other than above, all are Nominal features.


---


**FYI:**

**Nominal Feature**: Categories have no inherent order.

Examples: Colors, Country names.
Analysis: Suitable for categorical data, often one-hot encoded.

**Ordinal Feature**: Categories have a meaningful order.

Examples: Education levels, Survey responses.
Analysis: Order matters, intervals may not be uniform.

**Sparse Category Occurrence Chart -MF**.
--
"""

# # Count(Occurence) - plot of Categorical variables - MF

# for col in mf_small_cat.columns:
#     plt.figure(figsize=(8, 5), dpi=101, edgecolor="black")
#     plt.subplots_adjust(hspace=0.7, wspace=0.6)
#     sns.countplot(x=col, data=mf_small_cat, orient="v", palette="Set2", saturation=0.85)
#     plt.show()
#

for col in mf_small_cat.columns:
    plt.figure(figsize=(8, 5), dpi=101, edgecolor="black")
    plt.subplots_adjust(hspace=0.7, wspace=0.6)
    sns.countplot(x=col, data=mf_small_cat, orient="v", palette="viridis", saturation=0.8)
    plt.title(f"Countplot for {col}", fontsize=12, fontweight="bold")
    plt.xlabel(col, fontsize=10)
    plt.ylabel("Count", fontsize=10)
    sns.despine()
    plt.show()

"""**Observation:**

**Fund Ratings:**

The majority of funds are commonly classified as neutral, followed by bronze, and subsequently silver.


**Dividend Payouts:**

Dividend distributions among funds predominantly occur on an annual basis, with quarterly payouts also prevalent.


**Investment Styles:**

Within the fund landscape, a combination of value and growth-oriented funds is observed, with a notable prevalence of blend funds.



**Market Capitalization:**

Large-cap funds exhibit a clear dominance when compared to their small and medium-cap counterparts.


**Bond-Related Insights:**

Notably, there is a heightened sensitivity of funds to changes in bond interest rates.
Funds with low bond credit quality surpass those with medium quality, while high-quality funds are relatively less common.


**Currency Composition:**

The primary fund size currencies are USD, EUR, and GBP, signifying their dominance within the dataset.

** *italicized text*Abundant Category Occurrence Plot-MF**
--
"""

# for col in (mf_big_cat.columns):
#     plt.figure(figsize = (20,6), dpi = 90, edgecolor = "b")
#     plt.subplots_adjust(hspace = 0.7, wspace = 0.6)
#     sns.countplot(x = col, data = mf_big_cat, orient = "h", palette = "icefire" , saturation = 0.85 )
# plt.show()

for col in mf_big_cat.columns:
    plt.figure(figsize=(20, 6), dpi=90, edgecolor="b")
    sns.set(style="whitegrid")
    sns.countplot(x=col, data=mf_big_cat, orient="h", palette="icefire", saturation=0.85)
    plt.title(f"Countplot for {col}", fontsize=18, fontweight="bold")
    plt.xlabel("Count", fontsize=14)
    plt.ylabel(col, fontsize=14)
    plt.yticks(rotation=0, ha='right')
    plt.tight_layout()
    sns.despine()
    plt.show()

# Variables "morningstar_benchmark", "involment_palm_oil", "involvment_gmo", "sustanability_percentage_rank" has too many claases to fit in x - axis.
# 2. There 2 ways to deal with this.
#     - Sort only top 10 or 15 most frequent class and then plot that most frequent classes whiich surely enhances the readibility quotient.
#     - Plot the classes on Y - axis.

"""Charts of category variables unique counts equals to and less than 5 -MF

> Indented block


---

"""

fig = plt.figure(figsize = (11,6))
plt.title("Distribution of Rating Variable", size = 18,  fontweight = 10, pad = '2.0', loc = "center",
         fontstyle = 'italic', color = 'w',backgroundcolor = 'black')
df_mf["morningstar_rating"].value_counts().plot(kind = "pie", autopct = "%1.1f%%", cmap = 'Pastel2',
                                   textprops={'fontsize': 16, 'weight' : "bold"})
plt.ylabel('Rating', fontsize = 15, fontweight = "bold");

fig = plt.figure(figsize = (11,6))
plt.title("Distribution of Risk Rating Variable", size = 18,  fontweight = 10, pad = '2.0', loc = "right",
         fontstyle = 'italic', color = 'w',backgroundcolor = 'black')
df_mf["morningstar_risk_rating"].value_counts().plot(kind = "pie", autopct = "%1.1f%%", cmap = 'Pastel2',
                                   textprops={'fontsize': 16, 'weight' : "bold"})
plt.ylabel('Risk Rating', fontsize = 15, fontweight = "bold");

fig = plt.figure(figsize = (11,6))
plt.title("Distribution of Performance Rating Variable", size = 18,  fontweight = 10, pad = '2.0', loc = "right",
         fontstyle = 'italic', color = 'w',backgroundcolor = 'black')
df_mf["morningstar_performance_rating"].value_counts().plot(kind = "pie", autopct = "%1.1f%%", cmap = 'Pastel2',
                                   textprops={'fontsize': 16, 'weight' : "bold"})
plt.ylabel('Performance Rating', fontsize = 15, fontweight = "bold");

fig = plt.figure(figsize = (11,6))
plt.title("Distribution of Equity Style Variable", size = 18,  fontweight = 10, pad = '2.0', loc = "right",
         fontstyle = 'italic', color = 'w',backgroundcolor = 'black')
df_mf["equity_style"].value_counts().plot(kind = "pie", autopct = "%1.1f%%", cmap = 'Accent',
                                   textprops={'fontsize': 16, 'color': "w", 'weight' : "bold"})
plt.ylabel('Equity Style', fontsize = 15, fontweight = "bold");

fig = plt.figure(figsize = (11,6))
plt.title("Distribution of Equity Size Variable", size = 18,  fontweight = 10, pad = '2.0', loc = "right",
         fontstyle = 'italic', color = 'w',backgroundcolor = 'black')
df_mf["equity_size"].value_counts().plot(kind = "pie", autopct = "%1.1f%%", cmap = 'Set3',
                                   textprops={'fontsize': 16, 'weight' : "bold"})
plt.ylabel('Equity Size', fontsize = 15, fontweight = "bold");

fig = plt.figure(figsize = (11,6))
plt.title("Distribution of Bond Interest Rate Sensitivity Variable", size = 18,  fontweight = 10, pad = '2.0',
          loc = "right", fontstyle = 'italic', color = 'w',backgroundcolor = 'black')
df_mf["bond_interest_rate_sensitivity"].value_counts().plot(kind = "pie", autopct = "%1.1f%%", cmap = 'Set3',
                                   textprops={'fontsize': 16, 'weight' : "bold"})
plt.ylabel('Bond Interest Rate Sensitivity', fontsize = 15, fontweight = "bold");

fig = plt.figure(figsize = (11,6))
plt.title("Distribution of Bond Credit Quality Variable", size = 18,  fontweight = 10, pad = '2.0', loc = "right",
         fontstyle = 'italic', color = 'w',backgroundcolor = 'black')
df_mf["bond_credit_quality"].value_counts().plot(kind = "pie", autopct = "%1.1f%%", cmap = 'tab20c',
                                   textprops={'fontsize': 16, 'weight' : "bold"})
plt.ylabel('Bond Credit Quality', fontsize = 15, fontweight = "bold");

fig = plt.figure(figsize = (11,6))
plt.title("Distribution of Sustainability Rank Variable", size = 18,  fontweight = 10, pad = '2.0', loc = "right",
         fontstyle = 'italic', color = 'w',backgroundcolor = 'black')
df_mf["sustainability_rank"].value_counts().plot(kind = "pie", autopct = "%1.1f%%", cmap = 'Dark2',
                                   textprops={'fontsize': 16, 'weight' : "bold"})
plt.ylabel('Sustainability Rank', fontsize = 15, fontweight = "bold");

"""**Numerical Features**
---
Scatter plots are a valuable visualization tool, particularly for exploring relationships between two numerical variables.

The overall pattern of a scatterplot can be described by the direction, form, and strength of the relationship. An important kind of departure is an outlier, an individual value that falls outside the overall pattern of the relationship. Scatter plot is the visual reprsentation of Correlation matrix.

"""

corr_matrix = df_mf.corr(method = "pearson").round(2)
corr_matrix.style.background_gradient(cmap = 'YlGnBu')

corr_matrix_df = corr_matrix.unstack()
corr_matrix = corr_matrix[abs(corr_matrix) >= 0.8]

# print(corr_matrix)

corr_matrix.style.background_gradient(cmap = 'vlag')

# Correlation of Variables (Cross - checking of above Correlation matrix)

print(df_mf['morningstar_rating'].corr(df_mf['morningstar_performance_rating'])) # ---> 0.93
print(df_mf['roa'].corr(df_mf['roe']))  # ---> 0.91
print(df_mf['roa'].corr(df_mf['roic'])) # ---> 0.96
print(df_mf['roe'].corr(df_mf['roic'])) # ---> 0.88
print(df_mf['asset_stock'].corr(df_mf['asset_bond'])) # ---> -0.83
print(df_mf['ongoing_cost'].corr(df_mf['management_fees'])) # ---> 0.88
print(df_mf['environmental_score'].corr(df_mf['social_score'])) # ---> 0.81
print(df_mf['environmental_score'].corr(df_mf['governance_score'])) # ---> 0.83
print(df_mf['sustainability_rank'].corr(df_mf['sustainability_percentage_rank'])) # ---> -0.91
print(df_mf['involvement_abortive_contraceptive'].corr(df_mf['involvement_animal_testing'])) # ---> 0.82
print(df_mf['fund_return_2018'].corr(df_mf['fund_return_2018'])) # ---> 0.88
print(df_mf['fund_return_2017'].corr(df_mf['fund_return_2017'])) # ---> 0.81

plt.figure(figsize = (18,15))
plt.title("MF - Correlation HeatMap with Threshold limit >= 0.80", fontsize = 30)
correlation = df_mf_num.corr()
sns.heatmap(correlation, mask = correlation < 0.8, linewidth = 1.0, cmap = "YlGnBu", annot = False)

plt.figure(figsize = (18,15))
plt.title("MF - Correlation HeatMap with Threshold limit >= 0.80", fontsize = 30)
correlation = df_mf_num.corr()
sns.heatmap(correlation, mask = correlation < 0.8, linewidth = 1.0, cmap = "YlGnBu", annot = False)

"""Missing values

*   df.isnull().sum()


*   df.isna().sum()


*   df.notnull().sum()


*   df.isnull().sum().sum()


*   df.isnull().mean() * 100


*   percent_missing = df.isnull().sum() * 100 / len(df)


*   percent_missing = df.isnull().sum().sort_values(ascending = False) * 100 / len(df)


*   df.isnull().sum().sort_values(ascending = False)


*   df.isna().sum().sort_values(ascending = False)


*   Missing Value Visualization - Bar Chart

*   Missing Value Visualization - Dendogram
"""

df_mf[df_mf.columns[~df_mf.isnull().any()]].iloc[0:5, :].info()  # ---> Utilized the functionality of Negation.

# This are features having no missing values.
# As observed these features are very less

# Create a DataFrame of features having Missing Values - MF
# So we drop features with all values.
df_mf_missing = df_mf.loc[:, df_mf.columns.drop(["ticker", "isin", "fund_name",
                                                 "morningstar_category", "asset_stock", "asset_bond",
                                                 "asset_cash", "asset_other",
                                                 "nav_per_share_currency",
                                                 "nav_per_share", ])]

# Defining percent_missing_mf
percent_missing_mf = ((df_mf_missing.isnull().sum() / len (df_mf_missing)) * 100).sort_values(ascending = False)

print("\nMost values missing")
display(percent_missing_mf.nlargest(10))


print("\nleast values missing")
display(percent_missing_mf.nsmallest(10))


print("\n50 to 75 percentile values missing")
display(percent_missing_mf[50:75])

# Cross- Checking of DataFrame  - MF

display(df_mf.shape,
        df_mf_missing.shape)

# Least frequent value
print("\nLeast Frequent Value")
display(percent_missing_mf.value_counts().nsmallest(5))

# Least frequent value
print("\nMost Frequent Value")
display(percent_missing_mf.value_counts().nlargest(5))

# Assuming 'percent_missing_mf' is a pandas Series
percent_missing_mf = ((df_mf_missing.isnull().sum() / len(df_mf_missing)) * 100).sort_values(ascending=False)

# Create a DataFrame with the desired information
table_data = {
    '> 90%': [percent_missing_mf[percent_missing_mf > 90].count()],
    '> 85%': [percent_missing_mf[percent_missing_mf > 85].count()],
    '> 80%': [percent_missing_mf[percent_missing_mf > 80].count()],
    '> 75%': [percent_missing_mf[percent_missing_mf > 75].count()],
    '> 70%': [percent_missing_mf[percent_missing_mf > 70].count()],
    '> 65%': [percent_missing_mf[percent_missing_mf > 65].count()],
    '> 60%': [percent_missing_mf[percent_missing_mf > 60].count()],
    '> 55%': [percent_missing_mf[percent_missing_mf > 55].count()],
    '> 50%': [percent_missing_mf[percent_missing_mf > 50].count()],
    '> 45%': [percent_missing_mf[percent_missing_mf > 45].count()],
    '> 40%': [percent_missing_mf[percent_missing_mf > 40].count()],
    '> 35%': [percent_missing_mf[percent_missing_mf > 35].count()],
    '> 30%': [percent_missing_mf[percent_missing_mf > 30].count()],
    '> 25%': [percent_missing_mf[percent_missing_mf > 25].count()],
    '> 20%': [percent_missing_mf[percent_missing_mf > 20].count()],
    '> 15%': [percent_missing_mf[percent_missing_mf > 15].count()],
    '> 10%': [percent_missing_mf[percent_missing_mf > 10].count()],
    '> 5%': [percent_missing_mf[percent_missing_mf > 5].count()],
    '> 3%': [percent_missing_mf[percent_missing_mf > 3].count()],
    '> 2%': [percent_missing_mf[percent_missing_mf > 2].count()],
    '> 1%': [percent_missing_mf[percent_missing_mf > 1].count()],
    '< 1%': [percent_missing_mf[percent_missing_mf < 1].count()]
}

# Create a DataFrame
table_df = pd.DataFrame(table_data, index=['Count'])

# Display the table
print(table_df)

'''High Missing Values:

15 columns have over 90% missing data, indicating substantial gaps.
Incremental Trend:

The count of columns with missing values rises consistently as the threshold decreases.
Data Integrity Concerns:

A notable portion of the dataset (38 columns) has over 45% missing values, raising questions about data quality.
Critical Threshold:

5 columns have less than 1% missing values, suggesting a small but relatively complete subset.
Decision Point:

Careful consideration is needed for imputation or removal of columns with high missing values.
Investigation Needed:

Further exploration is warranted for columns with lower percentages to understand missing data patterns.'''

# Total values accross DataFrame
display(df_mf.isnull().sum().sum())
display(df_mf.isna().sum().sum())
display(df_mf.notnull().sum().sum())
display(df_mf.notna().sum().sum())

# Total values accoss df missing values.
display(df_mf_missing.isnull().sum().sum()) # ---> 2191765
display(df_mf_missing.isna().sum().sum()) #---> 2191765
display(df_mf_missing.notnull().sum().sum())  # --->  4605389
display(df_mf_missing.notna().sum().sum())# --->  4605389

# 1. DataFrame ---> df_mf

display((df_mf.isnull().sum().sum() / df_mf.size * 100).round(2))  # ---> 28.83%
display((df_mf.notnull().sum().sum() / df_mf.size * 100).round(2))   # ---> 71.17%


# 2. DataFrame ---> df_mf_missing

display((df_mf_missing.isnull().sum().sum() / df_mf_missing.size * 100).round(2))  # ---> 32.25%
display((df_mf_missing.notnull().sum().sum() / df_mf_missing.size * 100).round(2))   # ---> 67.75%

# Now, Visualizing the Missing values - MF
msno.bar(df_mf_missing, color = 'dodgerblue' , fontsize = 18, sort = "ascending", )

msno.dendrogram(df_mf_missing, fontsize = 18, method = "average")

"""**Duplicate values**
--

"""

# Duplicate Values - For a DataFrame as a whole - MF

df_mf.duplicated().sum()

(~df_mf.duplicated()).sum()  # ---> 57603

(~df_mf.duplicated()).sum()  # ---> 57603

df_mf[df_mf.duplicated('ticker')]  # ---> 0
df_mf[df_mf.duplicated('isin')] # ---> 2779
df_mf[df_mf.duplicated('fund_name')] # ---> 3596

"""*Distribution*, Skewness, Kurtosis
--

"""

def draw_histograms(df, variables, n_rows, n_cols):
    # Create subplots
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 2 * n_rows))
    axes = axes.flatten()

    # Iterate over variables
    for i, var_name in enumerate(variables):
        # Check if the index is within the bounds of axes
        if i < len(axes):
            # Create histogram for the current variable
            df[var_name].hist(bins=70, ax=axes[i], color='green', alpha=0.5)

            # Customize subplot
            axes[i].set_title(var_name, fontsize=15)
            axes[i].tick_params(axis='both', which='major', labelsize=12)
            axes[i].tick_params(axis='both', which='minor', labelsize=12)
            axes[i].set_xlabel('')

    # Adjust layout
    fig.tight_layout()
    plt.show()

# Adjust the figsize and font sizes according to your preferences
draw_histograms(df_mf_num, df_mf_num.columns, 55, 2)

"""**kernel density estimate (KDE) plots**"""

# Distribution of Numercial features - MF

fig, axes = plt.subplots(nrows = 37, ncols = 3, figsize = (25, 180), sharex = False, sharey = False)
axes = axes.ravel()    # ---> array to 1D
cols = df_mf_num.columns[:]  # ---> create a list of dataframe columns to use

for col, ax in zip(cols, axes):
    data = df_mf_num        # ---> select the data
    sns.kdeplot(data = data, x = col, shade = True, ax = ax, color = "#5334b5")
    ax.set(title = f'Distribution of Feature: {col}', xlabel = None)

#fig.delaxes(axes[5])  # ---> delete the empty subplot

fig.tight_layout(pad = 3)
plt.show()

"""Skewness

The skewness (G1) of a dataset can be calculated using Fisher's method:

G1 = (n(n-1) / ((n-2)(n+1)(n+3))) * Σ((Xi - X̄) / s)³

Where:
- n is the number of observations,
- Xi is each individual data point,
- X̄ is the mean of the data,
- s is the standard deviation of the data.
"""

# Skewness of Numerical features - MF

(df_mf_num.skew(axis = 0 , skipna = True, numeric_only = False)).round(2)

df_mf_num.shape

# Skewness of Numerical features - MF
(df_mf_num.skew(axis = 0 , skipna = True, numeric_only = False)).nlargest(10)

(df_mf_num.skew(axis = 0 , skipna = True, numeric_only = False)).nsmallest(10)

display(((df_mf_num.skew(axis = 0 , skipna = True, numeric_only = False)).round(2) > 1.5).sum(),  # ---> 51 (Positive Skew)
((df_mf_num.skew(axis = 0 , skipna = True, numeric_only = False)).round(2) < - 1.5).sum()   # ---> 10 (Negative Skew)

)

# List of features whose skewness of Numerical Features with Positive Skew Threshold limit >1.5
df_mf_num[df_mf_num.columns[df_mf_num.skew() > 1.5]].info()

# List of features whose skewness of Numerical Features with Positive Skew Threshold limit < -1.5

df_mf_num[df_mf_num.columns[df_mf_num.skew() < -1.5]].info()

"""**Kurtosis**
---

The concept of **kurtosis** is closely related to the presence of outliers in a dataset.

Kurtosis is a statistical measure that describes the "tailedness" or the shape of the tails of a probability distribution. It provides information about the concentration of data points in the tails compared to a normal distribution.

$$
\gamma_2 = \frac{\frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^4}{\left(\frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2\right)^2} - 3
$$

Here:

\
- \( n \) is the number of observations in the dataset.
- \( X_i \) represents each individual data point.
- \( x̄ \) is the mean of the dataset.


The interpretation of kurtosis depends on its value:
\[
\begin{align*}
\gamma_2 = 0 & : \text{**Mesokurtotic**Indicates that the tails of the distribution are similar to that of a normal distribution (mesokurtic).} \\
\gamma_2 > 0 & : \text{**Leptokurtic**Indicates that the tails are heavier (more data in the tails) than a normal distribution, and the distribution is leptokurtic (heavy-tailed) .} \\
\gamma_2 < 0 & : \text{**Platikurtic**Indicates that the tails are lighter (less data in the tails) than a normal distribution, and the distribution is platykurtic (light-tailed).}
\end{align*}


In finance, kurtosis is used as a measure of financial risk. A large kurtosis is associated with a high level of risk for an investment because it indicates that there are high probabilities of extremely large and extremely small returns. On the other hand, a small kurtosis signals a moderate level of risk because the probabilities of extreme returns are relatively low.
"""

(df_mf_num.kurt(axis = 0 , skipna = True, numeric_only = False)).round(2)

# Kurtosis of Numerical Features with Threshold limit (No. of Features) (Leptokurtic) - MF

((df_mf_num.kurt(axis = 0 , skipna = True, numeric_only = False)).round(2) > 3).sum()  # ---> 79  (Leptokurtic)

# Kurtosis of Numerical Features with Threshold limit (List of features) (Leptokurtic) - MF

df_mf_num[df_mf_num.columns[df_mf_num.kurt() > 3]].info()

# Kurtosis of Numerical Features with Threshold limit (No. of Features) (Platikurtic) - MF

((df_mf_num.kurt(axis = 0 , skipna = True, numeric_only = False)).round(2) < 3).sum()  # ---> 31  (Platikurtic)

# Kurtosis of Numerical Features with Threshold limit (List of features) (Platikurtic) - MF

df_mf_num[df_mf_num.columns[df_mf_num.kurt() < 3]].info()

# Kurtosis of Numerical Features with Threshold limit (No. of Features) (Mesokurtotic) - MF

((df_mf_num.kurt(axis = 0 , skipna = True, numeric_only = False)).round(2) == 3).sum()  # ---> 0  (Mesokurtotic)

"""Variance

Constant Features (Variance = 0)


Quasi - Constant Features (Variance = 0.01)


Quasi - Constant Features (Variance = 0.02)


Quasi - Constant Features (Variance = 0.03)


Perfect Non - Constant Features (Variance = 1)
"""

#  Constant Features - MF

# Step - I ---> Importing scikit learn library
from sklearn.feature_selection import VarianceThreshold


# Step - II ---> Utilizing the library

constant_selector = VarianceThreshold(threshold = 0)


# Step - III ---> fit the object to the data

constant_selector.fit(df_mf_num)

# Step - IV --->  Get features which DOES NOT have the variance greater than the set threshold value = 0.

sum(constant_selector.get_support(indices = False)) # ---> 110. (Non - Constant features)

# Step - V --->  Get features which DOES have the variance greater than the set threshold value = 0.

#   Method - 1 (Using Negation ~)
sum(~constant_selector.get_support(indices = False)) # ---> 0 (Constant features)

# Method - 2  (Using list comprehension)
selected_cols = [column for column in df_mf_num.columns if column not in df_mf_num.columns[constant_selector.get_support()]]
selected_cols   # ---> None

# Observtion -

# 1. There are no Constant Features in the MF numerical df.

quasi_constant_selector = VarianceThreshold(threshold = 0.01)
# Step - III ---> fit the object to the data

quasi_constant_selector.fit(df_mf_num)

# Step - IV --->  Get features which DOES NOT have the variance greater than the set threshold value = 0.

sum(quasi_constant_selector.get_support(indices = False)) # ---> 110 (Non - Constant features)

# Step - V --->  Get features which DOES have the variance greater than the set threshold value = 0.

#  Method - 1 (Using Negation ~)

sum(~quasi_constant_selector.get_support(indices = False)) # ---> 0 (Constant features)

# Method - 2  (Using list comprehension)

selected_cols = [column for column in df_mf_num.columns if column not in
                 df_mf_num.columns[quasi_constant_selector.get_support()]]
selected_cols   # ---> None

# # Observtion -

# 1. There are no Quasi - Constant Features in the MF numerical df.

"""

```
```

**Perfect Non - Constant Features**

This means those features in which not even a single value gets repeated. All observation of that feature contains unique values.

For example if a particular dataframe has 100 observations (rows) and no. of unique values = 100, then I would call it a "Perfect non-constant" feature.


This is possible in case of index or Id like employee Id. Such feature doesn't contribute for prediction of Target Feature and deserves to be dropped."""

for col in df_mf.columns:
    if df_mf[col].nunique() == 57603:
        print(col)

"""**Outliers & Anomalies**

1.   Percentile Method
2.   IQR Method
3.   Box - Plot Method
"""

# 1. Outlier Detection - (Percentile Method) - MF
# df_mf.describe(percentiles = [0.01, 0.02, 0.03, 0.04, 0.05, 0.95, 0.96, 0.97, 0.98, 0.99]).T
df_mf.quantile([0.01, 0.02, 0.03, 0.04, 0.05, 0.95, 0.96, 0.97, 0.98, 0.99]).T

# 2. Outlier Detection - (IQR Method) (UDF) - MF

def detect_outliers(data:pd.DataFrame, col_name:str, p = 1.5) ->int:
    '''
    this function detects outliers based on 3 time IQR and
    returns the number of lower and uper limit and number of outliers respectively
    '''
    first_quartile = np.percentile(np.array(data[col_name].tolist()), 25)
    third_quartile = np.percentile(np.array(data[col_name].tolist()), 75)
    IQR = third_quartile - first_quartile

    upper_limit = third_quartile+(p*IQR)
    lower_limit = first_quartile-(p*IQR)
    outlier_count = 0

    for value in data[col_name].tolist():
        if (value < lower_limit) | (value > upper_limit):
            outlier_count +=1
    return lower_limit, upper_limit, outlier_count

# 2. Outlier Detection - (IQR Method) - MF

iqr = 2

print(f"Number of Outliers for {iqr}*IQR after Logarithmed\n")

total = 0
for col in df_mf_num.columns:
    if detect_outliers(df_mf_num, col)[2] > 0:
        outliers = detect_outliers(df_mf_num, col, iqr)[2]
        total += outliers
        print("{} outliers in '{}'".format(outliers,col))
print("\n{} OUTLIERS TOTALLY".format(total))

# 3. Outlier Detection - (Box - Plot Method) - MF

# ax = fig.add_subplot(111)
sns.set_style('darkgrid')
sns.set_context("notebook", font_scale = 1.6)
plt.suptitle("Univariate Box Plots of Numerical Feratures", fontsize = 15, fontweight = 'bold')

for col in df_mf_num.columns:
    plt.figure(figsize = (16, 3))       # ---> plots figure for each iteration
    sns.boxplot(df_mf_num[col], color = "#90EE90", saturation = 0.50, width = 0.5)

fig.tight_layout()
# plt.show()


# Observations